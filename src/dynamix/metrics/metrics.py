import torch as tc
import matplotlib.pyplot as plt
import numpy as np
from scipy.ndimage import gaussian_filter1d


"""
For evaluating attractor geometries, we use a state space measure Dstsp
based on the Kullback-Leibler (KL) divergence, which assesses the (mis)match between 
the ground truth spatial distribution of data points, p_true(x), and the distribution 
p_gen(x|z) of trajectory points freely generated by the inferred DSR model
"""

def calc_histogram(x, n_bins, min_, max_):
    dim_x = x.shape[1]  # number of dimensions

    coordinates = (n_bins * (x - min_) / (max_ - min_)).long()

    # discard outliers
    coord_bigger_zero = coordinates > 0
    coord_smaller_nbins = coordinates < n_bins
    inlier = coord_bigger_zero.all(1) * coord_smaller_nbins.all(1)
    coordinates = coordinates[inlier]

    size_ = tuple(n_bins for _ in range(dim_x))
    indices = tc.ones(coordinates.shape[0], device=coordinates.device)
    return tc.sparse_coo_tensor(coordinates.t(), indices, size=size_).to_dense()

def normalize_to_pdf_with_laplace_smoothing(histogram, n_bins, smoothing_alpha=10e-6):
    if histogram.sum() == 0:  # if no entries in the range
        pdf = None
    else:
        dim_x = len(histogram.shape)
        pdf = (histogram + smoothing_alpha) / (histogram.sum() + smoothing_alpha * n_bins ** dim_x)
    return pdf

def kullback_leibler_divergence(p1, p2):
    if p1 is None or p2 is None:
        kl = tc.tensor([float('nan')])
    else:
        kl = (p1 * tc.log(p1 / p2)).sum()
    return kl

def geometrical_misalignment(x_gen, x_true, n_bins=30):
    min_, max_ = x_true.min(0).values, x_true.max(0).values
    hist_gen = calc_histogram(x_gen, n_bins=n_bins, min_=min_, max_=max_)
    hist_true = calc_histogram(x_true, n_bins=n_bins, min_=min_, max_=max_)

    p_gen = normalize_to_pdf_with_laplace_smoothing(histogram=hist_gen, n_bins=n_bins)
    p_true = normalize_to_pdf_with_laplace_smoothing(histogram=hist_true, n_bins=n_bins)
    return kullback_leibler_divergence(p_true, p_gen).item()

def geometrical_misalignment_single(x_gen, x_true, n_bins=30):
    kl = []
    for i in range(x_true.shape[1]):
        min_, max_ = x_true[:,i:i+1].min(0).values, x_true[:,i:i+1].max(0).values
        x_gen_bounded = np.clip(x_gen[:,i:i+1], min_+0.5, max_-0.5)
        hist_gen = calc_histogram(x_gen_bounded, n_bins=n_bins, min_=min_, max_=max_)
        hist_true = calc_histogram(x_true[:,i:i+1], n_bins=n_bins, min_=min_, max_=max_)

        p_gen = normalize_to_pdf_with_laplace_smoothing(histogram=hist_gen, n_bins=n_bins)
        p_true = normalize_to_pdf_with_laplace_smoothing(histogram=hist_true, n_bins=n_bins)

        kl.append(kullback_leibler_divergence(p_true, p_gen).item())
    return kl


"""
To assess temporal agreement, the Hellinger distances (DH) between the power spectra 
of the true and generated time series is computed
"""

def compute_and_smooth_power_spectrum(x, smoothing):
    x_ =  (x - x.mean()) / x.std()
    fft_real = np.fft.rfft(x_)
    ps = np.abs(fft_real)**2 * 2 / len(x_)
    ps_smoothed = gaussian_filter1d(ps, smoothing)
    return ps_smoothed / ps_smoothed.sum()

def hellinger_distance(p, q):
    return np.sqrt(1-np.sum(np.sqrt(p*q)))

def temporal_misalignment(X_gen, X, smoothing=20):
    dists = []
    for i in range(X.shape[1]):
        ps = compute_and_smooth_power_spectrum(X[:, i], smoothing)
        ps_gen = compute_and_smooth_power_spectrum(X_gen[:, i], smoothing)
        dists.append(hellinger_distance(ps, ps_gen))
    return np.mean(dists)


"""
For evaluating the short term prediction error n steps ahead
"""

def prediction_error(X, X_gen, steps=10):
    return tc.mean(tc.sum(tc.abs(X[steps,:] - X_gen[steps,:])))



"""
For evaluating the short term prediction error using MASE
"""

def MASE(X_gen, X, steps=10, naive_lag=1, average=True):
    """
    Multivariate Mean Absolute Scaled Error (MASE).
    
    Parameters:
    - X: tensor of shape (T, D), actual values
    - X_gen: tensor of shape (T, D), predicted values
    - steps: number of time steps to consider in MASE computation
    - naive_lag: lag for the naive forecast (default = 1)
    - average: if True, return mean MASE over all variables; else return per-variable MASE

    Returns:
    - MASE score (scalar if average=True, else tensor of shape (D,))
    """
    # Check dimensions
    assert X.shape == X_gen.shape, "Shapes of X and X_gen must match"
    
    # Truncate steps if too long
    steps = min(steps, X.shape[0])
    
    # MASE between prediction and ground truth (per variable)
    mase = tc.mean(tc.abs(X[:steps] - X_gen[:steps]), dim=0)  # shape: (D,)
    
    # Naive forecast MASE: |X[t] - X[t - naive_lag]|
    naive_mase = tc.mean(tc.abs(X[naive_lag:] - X[:-naive_lag]), dim=0)  # shape: (D,)
    
    # Avoid division by zero: replace 0 with epsilon (or return inf)
    epsilon = 1e-8
    mase = mase / (naive_mase + epsilon)
    
    return mase.mean() if average else mase